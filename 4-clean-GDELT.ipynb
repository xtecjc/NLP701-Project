{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7819760",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T09:00:57.637656Z",
     "start_time": "2023-07-13T09:00:47.976026Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Libraries \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from pymongo import MongoClient\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from alphabet_detector import AlphabetDetector\n",
    "from spacy_language_detection import LanguageDetector\n",
    "\n",
    "password = ''\n",
    "mongod_restart_command = \"sudo -S systemctl restart mongod\"\n",
    "os.system('echo %s | %s' % (password, mongod_restart_command))\n",
    "\n",
    "tqdm.pandas()\n",
    "ad = AlphabetDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebd50b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T12:18:25.906533Z",
     "start_time": "2023-07-12T12:12:14.210114Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Models \n",
    "\n",
    "# # Language detection \n",
    "nlp_model = spacy.load(\"en_core_web_sm\")\n",
    "Language.factory(\"language_detector\", func = get_lang_detector)\n",
    "nlp_model.add_pipe('language_detector', last = True)\n",
    "\n",
    "huff_classifier = pipeline(\"text-classification\", model = \"Yueh-Huan/news-category-classification-distilbert\")\n",
    "\n",
    "politics_binary = SetFitModel.from_pretrained('politics_binary/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6648f97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T09:00:57.650663Z",
     "start_time": "2023-07-13T09:00:57.639963Z"
    },
    "code_folding": [
     2,
     18,
     26
    ]
   },
   "outputs": [],
   "source": [
    "# Functions \n",
    "\n",
    "def read_mongoDB(localhost, database, collection): \n",
    "    \n",
    "    # Making a Connection with MongoClient\n",
    "    client = MongoClient(\"mongodb://localhost:\" + localhost + \"/\")\n",
    "    \n",
    "    # Database\n",
    "    db = client[database]\n",
    "    \n",
    "    # Collection\n",
    "    col = db[collection]\n",
    "    \n",
    "    data = pd.DataFrame(list(col.find()))\n",
    "    result = data.drop(\"_id\", axis = 1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def language_detection(text, nlp_model): \n",
    "\n",
    "    # Document level language detection\n",
    "    doc = nlp_model(text)\n",
    "    language = doc._.language\n",
    "    \n",
    "    return language\n",
    "\n",
    "def get_lang_detector(nlp, name): \n",
    "    \n",
    "    return LanguageDetector(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4e239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T07:19:43.654741Z",
     "start_time": "2023-07-13T07:19:43.645581Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# File paths \n",
    "\n",
    "# Get all the files \n",
    "directory = 'NLP701 Project'\n",
    "all_files = os.listdir(path = directory)\n",
    "\n",
    "# Get data files \n",
    "data_files = [directory + '/' + file for file in all_files if 'fthr' in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d3677c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-11T13:26:07.712734Z",
     "start_time": "2023-07-11T13:25:59.827544Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get the scraped data \n",
    "\n",
    "# Saudi scraped \n",
    "saudi_scraped = pd.read_feather(data_files[1])\n",
    "\n",
    "# Vietnam scraped \n",
    "vietnam_scraped = pd.read_feather(data_files[2])\n",
    "vietnam_scraped = vietnam_scraped[vietnam_scraped['lang'] == 'en'].drop(['lang'], axis = 1)\n",
    "\n",
    "# Afghanistan scraped \n",
    "afghanistan_scraped = read_mongoDB('27017', 'NLP701 Project', 'SCRAPED_ARTICLES')\n",
    "\n",
    "# Combine dataframes \n",
    "scraped_data = pd.concat([saudi_scraped, vietnam_scraped, afghanistan_scraped])\n",
    "\n",
    "# Deduplicate \n",
    "scraped_data = scraped_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f251d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T07:33:03.516319Z",
     "start_time": "2023-07-13T07:33:02.360334Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Saudi GDELT \n",
    "\n",
    "saudi_gdelt = pd.read_feather(data_files[-1])\n",
    "saudi_gdelt = saudi_gdelt[['Actor1CountryCode', 'Actor2CountryCode', 'IsRootEvent', 'EventCode',\n",
    "                           'EventBaseCode', 'EventRootCode', 'QuadClass', 'GoldsteinScale', 'NumMentions',\n",
    "                           'NumSources', 'NumArticles', 'AvgTone', 'DATEADDED', 'SOURCEURL']]\n",
    "\n",
    "saudi_gdelt = saudi_gdelt.dropna()\n",
    "saudi_gdelt = saudi_gdelt[saudi_gdelt['IsRootEvent'] == 1]\n",
    "saudi_gdelt = saudi_gdelt[((saudi_gdelt['GoldsteinScale'] > 0) & (saudi_gdelt['AvgTone'] > 0))|\n",
    "                          ((saudi_gdelt['GoldsteinScale'] < 0) & (saudi_gdelt['AvgTone'] < 0))]\n",
    "np.save('saudi_filtered_links.npy', saudi_gdelt['SOURCEURL'].unique())\n",
    "\n",
    "Vietnam scraped \n",
    "vietnam_gdelt = pd.read_feather(data_files[3])\n",
    "vietnam_gdelt = vietnam_gdelt[['Actor1CountryCode', 'Actor2CountryCode', 'IsRootEvent', 'EventCode',\n",
    "                               'EventBaseCode', 'EventRootCode', 'QuadClass', 'GoldsteinScale', 'NumMentions',\n",
    "                               'NumSources', 'NumArticles', 'AvgTone', 'DATEADDED', 'SOURCEURL']]\n",
    "\n",
    "vietnam_gdelt = vietnam_gdelt.dropna()\n",
    "vietnam_gdelt = vietnam_gdelt[vietnam_gdelt['IsRootEvent'] == 1]\n",
    "vietnam_gdelt = vietnam_gdelt[((vietnam_gdelt['GoldsteinScale'] > 0) & (vietnam_gdelt['AvgTone'] > 0))|\n",
    "                              ((vietnam_gdelt['GoldsteinScale'] < 0) & (vietnam_gdelt['AvgTone'] < 0))]\n",
    "np.save('vietnam_filtered_links.npy', vietnam_gdelt['SOURCEURL'].unique())\n",
    "\n",
    "afghanistan_gdelt = read_mongoDB('27017', 'GDELT', 'Afghanistan')\n",
    "afghanistan_gdelt = afghanistan_gdelt[['Actor1CountryCode', 'Actor2CountryCode', 'IsRootEvent', 'EventCode',\n",
    "                           'EventBaseCode', 'EventRootCode', 'QuadClass', 'GoldsteinScale', 'NumMentions',\n",
    "                           'NumSources', 'NumArticles', 'AvgTone', 'DATEADDED', 'SOURCEURL']]\n",
    "\n",
    "afghanistan_gdelt = afghanistan_gdelt.dropna()\n",
    "afghanistan_gdelt = afghanistan_gdelt[afghanistan_gdelt['IsRootEvent'] == 1]\n",
    "afghanistan_gdelt = afghanistan_gdelt[((afghanistan_gdelt['GoldsteinScale'] > 0) & (afghanistan_gdelt['AvgTone'] > 0))|\n",
    "                          ((afghanistan_gdelt['GoldsteinScale'] < 0) & (afghanistan_gdelt['AvgTone'] < 0))]\n",
    "\n",
    "np.save('afghanistan_filtered_links.npy', afghanistan_gdelt['SOURCEURL'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5125c4",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d272ec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T06:34:33.250790Z",
     "start_time": "2023-07-13T06:34:25.330416Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get the scraped data \n",
    "\n",
    "# File paths \n",
    "\n",
    "# Get all the files \n",
    "directory = 'NLP701 Project'\n",
    "all_files = os.listdir(path = directory)\n",
    "\n",
    "# Get data files \n",
    "data_files = [directory + '/' + file for file in all_files if 'fthr' in file]\n",
    "\n",
    "# Saudi scraped \n",
    "saudi_scraped = pd.read_feather(data_files[1])\n",
    "\n",
    "# Vietnam scraped \n",
    "vietnam_scraped = pd.read_feather(data_files[2])\n",
    "vietnam_scraped = vietnam_scraped[vietnam_scraped['lang'] == 'en'].drop(['lang'], axis = 1)\n",
    "\n",
    "# Afghanistan scraped \n",
    "afghanistan_scraped = read_mongoDB('27017', 'GDELT', 'AfghanistanArticles')\n",
    "\n",
    "# Combine dataframes \n",
    "scraped_data = pd.concat([saudi_scraped, vietnam_scraped, afghanistan_scraped])\n",
    "\n",
    "# Deduplicate \n",
    "scraped_data = scraped_data.dropna()\n",
    "\n",
    "scraped_data = scraped_data.drop(['PostDate'], axis = 1)\n",
    "scraped_data = scraped_data.set_index('SOURCEURL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e79a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T06:34:40.868662Z",
     "start_time": "2023-07-13T06:34:37.611994Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Read scraped data \n",
    "\n",
    "afghanistan_gdelt_scraped = pd.read_feather('afghanistan_gdelt_scraped.fthr')\n",
    "afghanistan_gdelt_scraped['GoldsteinScale'] = afghanistan_gdelt_scraped['GoldsteinScale'].astype(float)\n",
    "afghanistan_mean_goldstein = afghanistan_gdelt_scraped[['GoldsteinScale', 'SOURCEURL']].groupby('SOURCEURL').mean()\n",
    "\n",
    "saudi_gdelt_scraped = pd.read_feather('saudi_gdelt_scraped.fthr')\n",
    "saudi_mean_goldstein = saudi_gdelt_scraped[['GoldsteinScale', 'SOURCEURL']].groupby('SOURCEURL').mean()\n",
    "\n",
    "vietnam_gdelt_scraped = pd.read_feather('vietnam_gdelt_scraped.fthr')\n",
    "vietnam_mean_goldstein = vietnam_gdelt_scraped[['GoldsteinScale', 'SOURCEURL']].groupby('SOURCEURL').mean()\n",
    "\n",
    "mean_goldstein = pd.concat([afghanistan_mean_goldstein, saudi_mean_goldstein, vietnam_mean_goldstein], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f51e4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T06:34:50.140715Z",
     "start_time": "2023-07-13T06:34:48.896094Z"
    },
    "code_folding": [
     10,
     247
    ]
   },
   "outputs": [],
   "source": [
    "# Clean scraped data \n",
    "\n",
    "scraped_goldstein = mean_goldstein.merge(scraped_data, left_index = True, right_index = True)\n",
    "\n",
    "scraped_goldstein['Text'] = scraped_goldstein[['Title', 'Text']].agg('. '.join, axis = 1)\n",
    "\n",
    "scraped_goldstein = scraped_goldstein.drop(['Title'], axis = 1)\n",
    "\n",
    "scraped_goldstein = scraped_goldstein[scraped_goldstein['Text'].progress_apply(lambda x: ad.only_alphabet_chars(x, \"LATIN\"))]\n",
    "\n",
    "scraped_goldstein = scraped_goldstein[scraped_goldstein['Text'].progress_apply(lambda x: all([elm not in x for elm in ['é', 'è', 'ê', 'ë', 'ç', 'ñ', 'ø', 'ð', 'Ð ', 'å', 'æ', 'œ', 'ē', 'č', 'ŭ',  'š', 'ò',\n",
    " 'Ó',\n",
    " 'á',\n",
    " 'ó',\n",
    " 'ə',\n",
    " 'İ',\n",
    " 'ş',\n",
    " 'Ş',\n",
    " 'ı',\n",
    " 'ğ',\n",
    " 'ù',\n",
    " 'ä',\n",
    " 'ü',\n",
    " 'ö',\n",
    " 'ß',\n",
    " 'ł',\n",
    " 'ń',\n",
    " 'ż',\n",
    " 'ś',\n",
    " 'ę',\n",
    " 'ą',\n",
    " 'ć',\n",
    " 'ï',\n",
    " 'í',\n",
    " 'ú',\n",
    " 'ư',\n",
    " 'ơ',\n",
    " 'ô',\n",
    " 'ệ',\n",
    " 'ầ',\n",
    " 'ọ',\n",
    " 'ậ',\n",
    " 'ộ',\n",
    " 'ờ',\n",
    " 'â',\n",
    " 'Đ',\n",
    " 'ễ',\n",
    " 'ạ',\n",
    " 'ì',\n",
    " 'ấ',\n",
    " 'Â',\n",
    " 'ẩ',\n",
    " 'ả',\n",
    " 'Ô',\n",
    " 'ỗ',\n",
    " 'à',\n",
    " 'ồ',\n",
    " 'ề',\n",
    " 'ĩ',\n",
    " 'ố',\n",
    " 'ị',\n",
    " 'ă',\n",
    " 'ế',\n",
    " 'Ö',\n",
    " 'Ü',\n",
    " 'Ä',\n",
    " 'ž',\n",
    " 'ã',\n",
    " 'ș',\n",
    " 'ţ',\n",
    " 'î',\n",
    " 'Î',\n",
    " 'ț',\n",
    " 'Ș',\n",
    " 'Ţ',\n",
    " 'đ',\n",
    " 'Č',\n",
    " 'Ž',\n",
    " 'Ú',\n",
    " 'Á',\n",
    " 'É',\n",
    " 'Í',\n",
    " 'Ț',\n",
    " 'ớ',\n",
    " 'Ç',\n",
    " 'Ğ',\n",
    " 'ﬁ',\n",
    " 'Ă',\n",
    " 'Ã',\n",
    " 'ō',\n",
    " 'ừ',\n",
    " 'ň',\n",
    " 'ý',\n",
    " 'Ø',\n",
    " 'ﬂ',\n",
    " 'ḵ',\n",
    " 'õ',\n",
    " 'Õ',\n",
    " 'ȃ',\n",
    " 'ī',\n",
    " 'ā',\n",
    " 'ū',\n",
    " 'È',\n",
    " 'ų',\n",
    " 'ė',\n",
    " 'į',\n",
    " 'ở',\n",
    " 'ǎ',\n",
    " 'Ḳ',\n",
    " 'ļ',\n",
    " 'Ķ',\n",
    " 'Ę',\n",
    " 'Ā',\n",
    " 'ď',\n",
    " 'Ě',\n",
    " 'Ł',\n",
    " 'Ą',\n",
    " 'Š',\n",
    " 'Ż',\n",
    " 'Ē',\n",
    " 'Ế',\n",
    " 'ĺ',\n",
    " 'ﬃ',\n",
    " 'ﬀ',\n",
    " 'ũ',\n",
    " 'À',\n",
    " 'Ộ',\n",
    " 'ẫ',\n",
    " 'ắ',\n",
    " 'ặ',\n",
    " 'ợ',\n",
    " 'ỹ',\n",
    " 'ứ',\n",
    " 'ự',\n",
    " 'ẽ',\n",
    " 'ằ',\n",
    " 'ữ',\n",
    " 'ụ',\n",
    " 'ổ',\n",
    " 'ẻ',\n",
    " 'Ï',\n",
    " 'ů',\n",
    " 'ě',\n",
    " 'ř',\n",
    " 'Ə',\n",
    " 'Ś',\n",
    " 'ź',\n",
    " 'Ń',\n",
    " 'ḥ',\n",
    " 'Ḥ',\n",
    " 'Å',\n",
    " 'ỳ',\n",
    " 'ể',\n",
    " 'ủ',\n",
    " 'ỏ',\n",
    " 'Ꜥ',\n",
    " 'Ū',\n",
    " 'ử',\n",
    " 'Ź',\n",
    " 'ẵ',\n",
    " 'ẹ',\n",
    " 'Ờ',\n",
    " 'Œ',\n",
    " 'Ạ',\n",
    " 'ỡ',\n",
    " 'ỉ',\n",
    " 'Û',\n",
    " 'û',\n",
    " 'ő',\n",
    " 'Ê',\n",
    " 'ť',\n",
    " 'Ố',\n",
    " 'ḫ',\n",
    " 'Ð',\n",
    " 'þ',\n",
    " 'ṅ',\n",
    " 'ṭ',\n",
    " 'Ć',\n",
    " 'Ư',\n",
    " 'Ñ',\n",
    " 'ṣ',\n",
    " 'Æ',\n",
    " 'ȇ',\n",
    " 'Ɵ',\n",
    " 'Ấ',\n",
    " 'ỷ',\n",
    " 'Ò',\n",
    " 'ƒ',\n",
    " 'ľ',\n",
    " 'ŕ',\n",
    " 'ħ',\n",
    " 'Ġ',\n",
    " 'Ħ',\n",
    " 'Ẩ',\n",
    " 'Į',\n",
    " 'Ầ',\n",
    " 'Ơ',\n",
    " 'Ệ',\n",
    " 'Ậ',\n",
    " 'ỵ',\n",
    " 'Ẵ',\n",
    " 'Ả',\n",
    " 'Ắ',\n",
    " 'Ị',\n",
    " 'Ũ',\n",
    " 'Ì',\n",
    " 'Ồ',\n",
    " 'Ỹ',\n",
    " 'ᴀ',\n",
    " 'ɴ',\n",
    " 'ᴍ',\n",
    " 'ɪ',\n",
    " 'ᴅ',\n",
    " 'Ý',\n",
    " 'Ṣ',\n",
    " 'ḍ',\n",
    " 'ĝ',\n",
    " 'Ù',\n",
    " 'Ō',\n",
    " 'ņ',\n",
    " 'ķ',\n",
    " 'ꞌ',\n",
    " 'ǀ',\n",
    " 'Ő',\n",
    " 'ṇ',\n",
    " 'ÿ',\n",
    " 'Ë',\n",
    " 'Ť',\n",
    " 'ŋ',\n",
    " 'ʏ',\n",
    " 'ᴏ',\n",
    " 'ᴜ',\n",
    " 'ʀ',\n",
    " 'ᴡ',\n",
    " 'ʟ',\n",
    " 'ᴠ',\n",
    " 'ᴇ',\n",
    " 'ʙ',\n",
    " 'ᴄ',\n",
    " 'ɡ']]))]\n",
    "\n",
    "scraped_goldstein['lang'] = scraped_goldstein['Text'].progress_apply(lambda x: language_detection(x, nlp_model)['language'])\n",
    "\n",
    "scraped_goldstein = pd.read_feather('en_scraped_goldstein.fthr')\n",
    "\n",
    "scraped_goldstein = scraped_goldstein[scraped_goldstein['lang'] == 'en']\n",
    "\n",
    "scraped_goldstein_good = scraped_goldstein[~((scraped_goldstein['Text'].apply(lambda x: x.lower()).str.contains('news') & \n",
    "                                              scraped_goldstein['Text'].apply(lambda x: x.lower()).str.contains('headline'))|\n",
    "                                             (scraped_goldstein['Text'].apply(lambda x: x.lower()).str.contains('top') & \n",
    "                                              scraped_goldstein['Text'].apply(lambda x: x.lower()).str.contains('news')))]\n",
    "\n",
    "scraped_goldstein_good = scraped_goldstein_good.drop_duplicates(subset = ['SOURCEURL'])\n",
    "\n",
    "predictions = politics_binary.predict(list(scraped_goldstein_good['Text']))\n",
    "\n",
    "scraped_goldstein_good['predictions'] = predictions\n",
    "\n",
    "scraped_goldstein_good = scraped_goldstein_good[scraped_goldstein_good['predictions'] == 1]\n",
    "\n",
    "scraped_goldstein_good.reset_index(drop = True).to_feather('scraped_goldstein_pol_classified.fthr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa005449",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T09:07:48.957745Z",
     "start_time": "2023-07-13T09:07:47.738445Z"
    }
   },
   "outputs": [],
   "source": [
    "scraped_goldstein_good = pd.read_feather('scraped_goldstein_pol_classified.fthr')\n",
    "\n",
    "saudi_links = list(np.load('saudi_filtered_links.npy', allow_pickle = True))\n",
    "\n",
    "vietnam_links = list(np.load('vietnam_filtered_links.npy', allow_pickle = True))\n",
    "\n",
    "afghanistan_links = list(np.load('afghanistan_filtered_links.npy', allow_pickle = True))\n",
    "\n",
    "all_links = saudi_links + vietnam_links + afghanistan_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41b41125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T09:08:02.176631Z",
     "start_time": "2023-07-13T09:08:01.790011Z"
    }
   },
   "outputs": [],
   "source": [
    "scraped_goldstein_good = scraped_goldstein_good[scraped_goldstein_good['SOURCEURL'].isin(all_links)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b590c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T03:29:59.974215Z",
     "start_time": "2023-07-13T03:29:59.969658Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get political subset \n",
    "\n",
    "texts = [elm[:2500] for elm in scraped_goldstein_good['Text'].unique()]\n",
    "\n",
    "# Links with political articles \n",
    "\n",
    "pol_not_dataset = []\n",
    "for text in tqdm(texts): \n",
    "    \n",
    "    result = huff_classifier.predict(text)[0]\n",
    "    result['text'] = text\n",
    "    \n",
    "    pol_not_dataset.append(result)\n",
    "\n",
    "pol_not_df = pd.DataFrame(pol_not_dataset)\n",
    "\n",
    "pol_not_df.to_feather('pol_not_df_2.fthr')\n",
    "\n",
    "not_pol = pol_not_df[~(pol_not_df['label'].isin(['WORLDPOST', 'BUSINESS', 'WORLD NEWS', 'POLITICS', 'THE WORLDPOST', 'IMPACT'])) & \n",
    "                      (pol_not_df['score'] > 0.9)]\n",
    "\n",
    "not_pol = not_pol.sort_values('score', ascending = False)\n",
    "\n",
    "pol = pol_not_df[(pol_not_df['label'].isin(['WORLDPOST', 'WORLD NEWS', 'POLITICS'])) & \n",
    "                 (pol_not_df['score'] > 0.75)]\n",
    "\n",
    "pol_list = []\n",
    "for topic in ['WORLDPOST', 'WORLD NEWS', 'POLITICS']: \n",
    "    \n",
    "    pol_filt = pol[pol['label'] == topic].sort_values('score', ascending = False).iloc[:3155]\n",
    "    pol_list.append(pol_filt)\n",
    "    \n",
    "pol = pd.concat(pol_list)\n",
    "pol = pol.sort_values('score', ascending = False)\n",
    "\n",
    "not_pol.reset_index(drop = True).to_feather('not_pol.fthr')\n",
    "\n",
    "pol.reset_index(drop = True).to_feather('pol.fthr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
