{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c261bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T19:47:59.232893Z",
     "start_time": "2023-10-31T19:47:58.755841Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Libraries \n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import http\n",
    "import requests\n",
    "\n",
    "# Libraries \n",
    "\n",
    "# Starting MongoDB\n",
    "password = '' # Your System Password\n",
    "mongod_restart_command = \"sudo -S systemctl restart mongod\"\n",
    "os.system('echo %s | %s' % (password, mongod_restart_command))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95e654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:39:04.695372Z",
     "start_time": "2023-10-31T18:39:01.560232Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Models \n",
    "\n",
    "# Read space NLP model \n",
    "spacy_nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db35562",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T21:08:03.774296Z",
     "start_time": "2023-10-31T21:08:03.529655Z"
    },
    "code_folding": [
     2,
     22,
     32,
     57,
     72,
     86,
     125,
     172,
     193,
     230,
     266,
     281,
     306,
     318,
     329,
     341,
     362
    ]
   },
   "outputs": [],
   "source": [
    "# Functions \n",
    "\n",
    "def query_id(col, id_object): \n",
    "    \n",
    "    raw_result = list(col.find({\"_id\": id_object}))[0]\n",
    "    \n",
    "    _id = raw_result['_id']\n",
    "    \n",
    "    url = raw_result['url']\n",
    "    \n",
    "    cat = raw_result['category']\n",
    "    \n",
    "    title = raw_result['title']\n",
    "    \n",
    "    text = raw_result['text']\n",
    "        \n",
    "    result = {'_id':_id, 'SOURCEURL':url, 'category':cat, 'Title':title, 'Text':text}\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Splitting functions  \n",
    "\n",
    "def paragraph_split(text): \n",
    "    \n",
    "    # Get paragraphs \n",
    "    pgs = re.split(\"\\n |\\n\\n\", text)\n",
    "    \n",
    "    # Strip whitespace \n",
    "    pgs_no_whitespace = [re.sub(' \\n', ' ', re.sub(' +', ' ', pg.strip())) for pg in pgs if pg.strip() != '']\n",
    "        \n",
    "    return pgs_no_whitespace\n",
    "\n",
    "def sentence_split(text): \n",
    "    \n",
    "    # Clean text \n",
    "    clean_text = text.replace('\\xa0', ' ')\n",
    "\n",
    "    # Separate sentences by internal newlines \n",
    "    paragraphs = [pg for pg in paragraph_split(clean_text) if len(pg.split(' ')) >= 2]\n",
    "\n",
    "    # Get sentences \n",
    "    sentences = []\n",
    "    for pg in paragraphs: \n",
    "\n",
    "        pg_sentences = sent_tokenize(pg)\n",
    "        valid_sentences = [sentence for sentence in pg_sentences if len(sentence.split(' ')) >= 2]\n",
    "        sentences.extend(valid_sentences)\n",
    "\n",
    "    # Split sentences by ; \n",
    "    semicolon_sentences = []\n",
    "    for sentence in sentences: \n",
    "\n",
    "        split_sentences = sentence.split('; ')\n",
    "        semicolon_sentences.extend(split_sentences)\n",
    "        \n",
    "    return semicolon_sentences\n",
    "\n",
    "def get_company_contract_sentences(company_name, company_ids): \n",
    "\n",
    "    contract_ids = company_ids[company_name]\n",
    "    sentences = []\n",
    "    for contract_id in contract_ids:\n",
    "        \n",
    "        contract_text = nlp_utils.query_id(col, contract_id)['Text']\n",
    "        if contract_text:\n",
    "            contract_sentences = sentence_split(contract_text)\n",
    "            sentences.extend(contract_sentences)\n",
    "        \n",
    "    return sentences\n",
    "\n",
    "# Triple extraction functions \n",
    "\n",
    "def is_valid_sentence(span): \n",
    "    \n",
    "    \"\"\"\n",
    "    Checks if a span of text has a subject and a verb, making it a valid sentence.\n",
    "\n",
    "    :param span: spacy.tokens.Span, the span to check\n",
    "    :return: bool, True if the span is a valid sentence, False otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    has_subject = any(['subj' in token.dep_ for token in span])\n",
    "    has_verb = any([token.pos_ in [\"AUX\", \"VERB\"] for token in span])\n",
    "    \n",
    "    return has_subject and has_verb\n",
    "\n",
    "def is_full_sentence(span): \n",
    "    \n",
    "    \"\"\"\n",
    "    Checks if a span of text has a subject and a verb, making it a valid sentence.\n",
    "\n",
    "    :param span: spacy.tokens.Span, the span to check\n",
    "    :return: bool, True if the span is a valid sentence, False otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    subject_ids = []\n",
    "    object_ids = []\n",
    "    verb_ids = []\n",
    "    root_id = None\n",
    "    for token in span:\n",
    "        \n",
    "        if 'subj' in token.dep_:\n",
    "            subject_ids.append(token.i)\n",
    "            \n",
    "        if token.pos_ in [\"AUX\", \"VERB\"]:\n",
    "            verb_ids.append(token.i)\n",
    "\n",
    "        if 'obj' in token.dep_:\n",
    "            object_ids.append(token.i)\n",
    "            \n",
    "        if token.dep_ == 'ROOT':\n",
    "            root_id = token.i\n",
    "            \n",
    "    if subject_ids != [] and verb_ids != [] and object_ids != [] and root_id: \n",
    "            \n",
    "        if any([id_ < root_id for id_ in subject_ids]) and any([id_ > root_id for id_ in object_ids]): \n",
    "\n",
    "            return True\n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def split_compound_sentences(doc): \n",
    "    \n",
    "    \"\"\"\n",
    "    Splits compound sentences into simple sentences.\n",
    "    \n",
    "    :param text: str, input compound sentence\n",
    "    :return: list[str], list of simple sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = []\n",
    "    # Iterate through tokens in the parsed sentence\n",
    "    for sent in doc.sents:\n",
    "        \n",
    "        start = sent.start\n",
    "        for token in sent:\n",
    "            \n",
    "            # Check for conjunctions and punctuation that typically link clauses\n",
    "            if token.dep_ in ['cc', 'mark'] or (token.dep_ == 'punct'):\n",
    "                \n",
    "                # Check is head of token is root and that it is a verb or verb modifier\n",
    "                if token.head.dep_ == \"ROOT\" or (token.head.pos_=='VERB' and token.head.dep_ in ['conj', 'advcl']):\n",
    "                    \n",
    "                    left_sentence = doc[start:token.i]\n",
    "                    right_sentence = doc[token.i:sent.end]\n",
    "\n",
    "                    # Check if the clause before the conjunction/comma forms a valid sentence\n",
    "                    if is_valid_sentence(left_sentence):\n",
    "                        \n",
    "                        if left_sentence not in sentences:\n",
    "                            sentences.append(left_sentence)                        \n",
    "                            start = token.i + 1\n",
    "                            \n",
    "                    if is_valid_sentence(right_sentence):\n",
    "                        \n",
    "                        if right_sentence not in sentences:\n",
    "                            sentences.append(right_sentence)                        \n",
    "                            start = token.i + 1\n",
    "                            \n",
    "    deduplicated_sentences = clean_list(sentences)\n",
    "    cleaned_sentences = [clean_sentence(sentence) for sentence in deduplicated_sentences]\n",
    "\n",
    "    full_sentences = [sentence for sentence in cleaned_sentences if is_full_sentence(sentence)]\n",
    "    if full_sentences == []:\n",
    "        full_sentences = [doc]\n",
    "    \n",
    "    return full_sentences\n",
    "\n",
    "def extract_expanded_subject(doc): \n",
    "    \n",
    "    subj_tokens = [token for token in doc if 'subj' in token.dep_ and token.head.dep_ == 'ROOT']\n",
    "    expanded_subjs = []\n",
    "    for subj_token in subj_tokens: \n",
    "        expanded_subj = [descendant for descendant in subj_token.subtree]\n",
    "        expanded_subjs.append(expanded_subj)\n",
    "    \n",
    "    if expanded_subjs == []:\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        full_subj_tokens = []\n",
    "        for expanded_subj in expanded_subjs: \n",
    "            full_subj_tokens.extend([token for token in expanded_subj])\n",
    "        \n",
    "        full_subj_tokens = sorted(full_subj_tokens, key = lambda x: x.i)\n",
    "\n",
    "        return full_subj_tokens\n",
    "\n",
    "def extract_expanded_verb(doc): \n",
    "    \n",
    "    root_tokens = [token for token in doc if token.dep_ == 'ROOT']\n",
    "    if len(root_tokens) == 1:\n",
    "        \n",
    "        root_token = root_tokens[0]\n",
    "        root_subtree = [token for token in root_token.subtree]\n",
    "        root_ids = [root_token.i]\n",
    "\n",
    "        left_subtree = [token for token in root_subtree if token.i < root_token.i]\n",
    "        sorted_left_subtree = sorted(left_subtree, key = lambda x: x.i)\n",
    "        relevant_left_subtree = []\n",
    "        for token in sorted_left_subtree[::-1]:\n",
    "            if (token.head.i in root_ids) and any([dep in token.dep_ for dep in ['agent','neg', 'aux', 'ccomp', 'xcomp', 'acomp', 'oprd', 'cc', 'conj', 'adv', 'prt']]):\n",
    "                relevant_left_subtree.append(token)\n",
    "                root_ids.append(token.i)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        right_subtree = [token for token in root_subtree if token.i > root_token.i]\n",
    "        sorted_right_subtree = sorted(right_subtree, key = lambda x: x.i)\n",
    "        relevant_right_subtree = []\n",
    "        for token in sorted_right_subtree:\n",
    "            if (token.head.i in root_ids) and any([dep in token.dep_ for dep in ['agent','neg', 'aux', 'ccomp', 'xcomp', 'acomp', 'oprd', 'cc', 'conj', 'adv', 'prt']]):\n",
    "                relevant_right_subtree.append(token)\n",
    "                root_ids.append(token.i)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        full_root = relevant_left_subtree + [root_token] + relevant_right_subtree\n",
    "        expanded_root_tokens = sorted(full_root, key = lambda x: x.i)\n",
    "        \n",
    "    else:\n",
    "        expanded_root_tokens = None\n",
    "        \n",
    "    return expanded_root_tokens\n",
    "\n",
    "def extract_expanded_object(doc, root): \n",
    "    \n",
    "    prep_tokens = [token for token in doc if any([dep in token.dep_ for dep in ['prep']]) and token.head.i in [token.i for token in root]]\n",
    "    expanded_prep_objs = []\n",
    "    for prep_token in prep_tokens: \n",
    "        expanded_prep_obj = [descendant for descendant in prep_token.subtree]\n",
    "        expanded_prep_objs.extend(expanded_prep_obj)\n",
    "    if expanded_prep_objs == []: \n",
    "        expanded_prep_objs = None\n",
    "    else: \n",
    "        expanded_prep_objs = list(np.unique(sorted(expanded_prep_objs, key = lambda x: x.i)))\n",
    "        \n",
    "    obj_tokens = [token for token in doc if any([dep in token.dep_ for dep in ['obj', 'obl', 'ccomp', 'acomp', 'attr', 'xcomp']]) and token.head.i in [token.i for token in root]]\n",
    "    expanded_obj_objs = []\n",
    "    for obj_token in obj_tokens: \n",
    "        expanded_obj_obj = [descendant for descendant in obj_token.subtree]\n",
    "        expanded_obj_objs.extend(expanded_obj_obj)\n",
    "    if expanded_obj_objs == []: \n",
    "        expanded_obj_objs = None\n",
    "    else: \n",
    "        expanded_obj_objs = list(np.unique(sorted(expanded_obj_objs, key = lambda x: x.i)))\n",
    "\n",
    "    advcl_tokens = [token for token in doc if any([dep in token.dep_ for dep in ['advcl']]) and token.head.i in [token.i for token in root]]\n",
    "    expanded_advcl_objs = []\n",
    "    for advcl_token in advcl_tokens: \n",
    "        expanded_advcl_obj = [descendant for descendant in advcl_token.subtree]\n",
    "        expanded_advcl_objs.extend(expanded_advcl_obj)  \n",
    "    if expanded_advcl_objs == []: \n",
    "        expanded_advcl_objs = None\n",
    "    else: \n",
    "        expanded_advcl_objs = list(np.unique(sorted(expanded_advcl_objs, key = lambda x: x.i)))\n",
    "\n",
    "    return expanded_prep_objs, expanded_obj_objs, expanded_advcl_objs\n",
    "\n",
    "def format_triples(all_triples): \n",
    "    \n",
    "    formatted_triples = []\n",
    "    for triples in tqdm(all_triples): \n",
    "            \n",
    "        for triple in triples: \n",
    "            \n",
    "            subj_str = ' '.join([token.text for token in triple['subj']])\n",
    "            rel_str = ' '.join([token.text for token in triple['rel']])\n",
    "            obj_str = ' '.join([token.text for token in triple['obj']])\n",
    "            \n",
    "            formatted_triples.append([subj_str, rel_str, obj_str])\n",
    "\n",
    "    return formatted_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe023c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T18:41:39.387288Z",
     "start_time": "2023-10-31T18:39:12.433007Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get text from DB \n",
    " \n",
    "# DB \n",
    "\n",
    "# Access database  \n",
    "client = MongoClient(\"mongodb://localhost:\" + \"27017\" + \"/\")\n",
    "\n",
    "# Access Database\n",
    "db = client['NLP701 Project']\n",
    "\n",
    "# Access Collection in Database\n",
    "col = db['GDELT_ARTICLES']\n",
    "\n",
    "doc_ids = col.distinct('_id')\n",
    "\n",
    "# Get companies and their contracts \n",
    "articles = {}\n",
    "article_ind = 0\n",
    "for _id in tqdm(doc_ids): \n",
    "    \n",
    "    datapoint = query_id(col, _id)\n",
    "    articles[article_ind] = datapoint\n",
    "    \n",
    "    article_ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44676ac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T04:12:02.584358Z",
     "start_time": "2023-10-31T21:56:07.545325Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "triples = {}\n",
    "for index, text in tqdm(articles.items()): # got up to 54\n",
    "    \n",
    "    sents = sent_tokenize(text)\n",
    "    all_triples = []\n",
    "    for sentence in tqdm(sents): \n",
    "\n",
    "        parsed_sentence = spacy_nlp(sentence)\n",
    "\n",
    "        simplified_sentences = split_compound_sentences(parsed_sentence)\n",
    "\n",
    "        triples = []\n",
    "        for doc in simplified_sentences: \n",
    "\n",
    "            text = capitalize_first(doc.text.strip()) + \".\"\n",
    "\n",
    "            doc = spacy_nlp(text)\n",
    "            subj = extract_expanded_subject(doc)    \n",
    "            rel = extract_expanded_verb(doc)\n",
    "\n",
    "            if subj and rel: \n",
    "\n",
    "                obj = None\n",
    "\n",
    "                modifiers = []\n",
    "                obj_tok, prep_tok, advcl_tok = extract_expanded_object(doc, rel)\n",
    "                if obj_tok != []: \n",
    "\n",
    "                    obj = obj_tok\n",
    "\n",
    "                    if prep_tok != []: \n",
    "                        prep_modifier = prep_tok\n",
    "                        modifiers.append(prep_modifier)\n",
    "\n",
    "                    if advcl_tok != []: \n",
    "                        advcl_modifier = advcl_tok\n",
    "                        modifiers.append(advcl_modifier)\n",
    "\n",
    "                elif (obj_tok == []) and (prep_tok != []): \n",
    "\n",
    "                    obj = prep_tok\n",
    "\n",
    "                    if advcl_tok != []: \n",
    "                        advcl_modifier = advcl_tok\n",
    "                        modifiers.append(advcl_modifier)\n",
    "\n",
    "                if obj:\n",
    "\n",
    "                    new_triple = {'sentence': doc, \n",
    "                                  'subj':subj, 'rel':rel, 'obj':obj, \n",
    "                                  'mods':modifiers}\n",
    "\n",
    "                    triples.append(new_triple)\n",
    "\n",
    "        if triples != []:\n",
    "            all_triples.append(triples)\n",
    "            \n",
    "    triples[company] = all_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd359c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T04:26:17.615910Z",
     "start_time": "2023-11-01T04:26:16.292063Z"
    }
   },
   "outputs": [],
   "source": [
    "formatted_triples = []\n",
    "for index, sentences in tqdm(triples.items()): \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        for triple in sentence:\n",
    "            \n",
    "            formatted_triple = {'sentence':triple['sentence'],\n",
    "                                'subj': [token.i for token in triple['subj']],\n",
    "                                'rel':[token.i for token in triple['rel']],\n",
    "                                'obj':[token.i for token in triple['obj']],\n",
    "                                'mods':[]}\n",
    "            \n",
    "            for mod in triple['mods']:\n",
    "                \n",
    "                if mod:\n",
    "                    formatted_mod = [token.i for token in mod]\n",
    "                    formatted_triple['mods'].append(formatted_mod)\n",
    "                    \n",
    "            formatted_triples.append(formatted_triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b669da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T04:49:17.211150Z",
     "start_time": "2023-11-01T04:48:38.519762Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('formatted_triples.pickle', 'wb') as handle:\n",
    "    \n",
    "    pickle.dump(formatted_triples, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
