{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4779024f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-07T11:24:42.226491Z",
     "start_time": "2023-07-07T11:24:41.127375Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Libraries \n",
    "\n",
    "import os\n",
    "import psutil\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from newsplease import NewsPlease\n",
    "\n",
    "from multiprocess import Pool\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "# Starting MongoDB\n",
    "password = '' # YOUR SYSTEM PASSWORD\n",
    "mongod_restart_command = \"sudo -S systemctl restart mongod\"\n",
    "os.system('echo %s | %s' % (password, mongod_restart_command))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75170df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T13:56:42.275866Z",
     "start_time": "2023-06-28T13:56:42.251638Z"
    },
    "code_folding": [
     2,
     32,
     48,
     62,
     66
    ]
   },
   "outputs": [],
   "source": [
    "# Functions \n",
    "\n",
    "def article_scraper(url): \n",
    "\n",
    "    try: \n",
    "        source = requests.get(url, timeout = 10)\n",
    "\n",
    "        article = NewsPlease.from_html(source.content)\n",
    "\n",
    "        article_dict = article.get_dict()\n",
    "\n",
    "        title = article_dict['title']\n",
    "        texts = [article_dict['description'], article_dict['maintext'], article_dict['text'], article_dict['description']]\n",
    "\n",
    "        scraped_date = article_dict['date_publish']\n",
    "        if scraped_date != None:\n",
    "            post_date = str(scraped_date.date())\n",
    "        else:\n",
    "            post_date = None\n",
    "\n",
    "        if all([elm == None for elm in texts]):\n",
    "            full_text = None\n",
    "        else:\n",
    "            full_text = \". \".join(\". \".join([text for text in texts if text != None]).splitlines())\n",
    "\n",
    "        results = {'SOURCEURL':url, 'Title':title, 'Text':full_text, 'PostDate':post_date}\n",
    "        \n",
    "    except: \n",
    "        results = None \n",
    "\n",
    "    return results\n",
    "\n",
    "def read_mongoDB(localhost, database, collection): \n",
    "    \n",
    "    # Making a Connection with MongoClient\n",
    "    client = MongoClient(\"mongodb://localhost:\" + localhost + \"/\")\n",
    "    \n",
    "    # Database\n",
    "    db = client[database]\n",
    "    \n",
    "    # Collection\n",
    "    col = db[collection]\n",
    "    \n",
    "    data = pd.DataFrame(list(col.find()))\n",
    "    result = data.drop(\"_id\", axis = 1)\n",
    "    \n",
    "    return result    \n",
    "    \n",
    "def insert_mongoDB(df, localhost, database, collection): \n",
    "    \n",
    "    # Making a Connection with MongoClient\n",
    "    client = MongoClient(\"mongodb://localhost:\" + localhost + \"/\")\n",
    "    \n",
    "    # Database\n",
    "    db = client[database]\n",
    "    \n",
    "    # Collection\n",
    "    col = db[collection]\n",
    "    \n",
    "    # Load to mongoDB \n",
    "    col.insert_many(df.to_dict('records'))\n",
    "    \n",
    "def get_pid(name): \n",
    "     \n",
    "    return check_output([\"pidof\",name])\n",
    "\n",
    "def process_ram_retrieve(process_name): \n",
    "    \n",
    "    mongod_process_id = int(get_pid(process_name).decode().strip())\n",
    "    mongod_process = psutil.Process(mongod_process_id)\n",
    "    process_ram = (mongod_process.memory_info().rss/1024 ** 2)\n",
    "        \n",
    "    return process_ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c716c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T13:56:57.213878Z",
     "start_time": "2023-06-28T13:56:53.943467Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Read data \n",
    "\n",
    "df = pd.read_feather('GDELT_DATA.fthr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b45bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T13:56:58.874871Z",
     "start_time": "2023-06-28T13:56:58.141650Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get links \n",
    "\n",
    "links = [link for link in df['SOURCEURL'].unique() if link]\n",
    "links = [link for link in links if 'http' in link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acf1c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T13:28:29.980487Z",
     "start_time": "2023-06-28T13:28:28.328481Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Remove scraped links \n",
    "\n",
    "scraped_df = read_mongoDB(\"27017\", \"NLP701_Project\", \"SCRAPED_ARTICLES\")\n",
    "os.system('echo %s | %s' % (password, mongod_restart_command))\n",
    "scraped_links = set(scraped_df['link'].unique())\n",
    "remaining_links = list(set(all_links).difference(scraped_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86826f21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T13:57:04.981820Z",
     "start_time": "2023-06-28T13:57:04.814625Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get chunk into parallels \n",
    "\n",
    "link_chunks = [links[i:i + 15] for i in range(0, len(links), 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfe6c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T14:01:21.310607Z",
     "start_time": "2023-06-28T13:59:03.594524Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parallel scrape all links \n",
    "\n",
    "num_chunk = 1\n",
    "p = Pool(15)\n",
    "for chunk in tqdm(link_chunks): \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Scrape in parallel \n",
    "        result = p.map_async(article_scraper, chunk)\n",
    "        data = result.get(timeout = None)\n",
    "        not_none = [elm for elm in data if elm]\n",
    "        \n",
    "        if data: \n",
    "            \n",
    "            # Convert to df\n",
    "            data_df = pd.DataFrame(not_none)\n",
    "        \n",
    "            # Load to MongoDB\n",
    "            insert_mongoDB(data_df, \"27017\", \"NLP701_Project\", \"SCRAPED_ARTICLES\")\n",
    "\n",
    "            # Check RAM by MongoDB\n",
    "            process_ram = process_ram_retrieve('mongod')\n",
    "            if process_ram > 1000: \n",
    "                \n",
    "                os.system('echo %s | %s' % (password, mongod_restart_command))\n",
    "                print(num_chunk)\n",
    "                \n",
    "    except KeyboardInterrupt: \n",
    "        \n",
    "        p.terminate()\n",
    "        p.join()\n",
    "        \n",
    "        break\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    num_chunk += 1\n",
    "    \n",
    "p.terminate()\n",
    "p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
