{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a3570d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T12:36:56.259674Z",
     "start_time": "2023-12-16T12:36:53.098573Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Libraries \n",
    "\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2bba5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T12:36:59.674811Z",
     "start_time": "2023-12-16T12:36:59.198296Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Models \n",
    "\n",
    "# Read nlp model\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0600757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T12:37:00.343857Z",
     "start_time": "2023-12-16T12:37:00.332205Z"
    },
    "code_folding": [
     2,
     10
    ]
   },
   "outputs": [],
   "source": [
    "# Functions \n",
    "\n",
    "def start_end_finder(string, substring): \n",
    "    \n",
    "    matches = re.finditer(substring, string)\n",
    "    \n",
    "    matches_info = [(match.start(), match.end()) for match in matches]\n",
    "    \n",
    "    return matches_info\n",
    "\n",
    "def get_span(doc, indices): \n",
    "    \n",
    "    # Ensure indices are provided\n",
    "    if not indices:\n",
    "        return (None, None)\n",
    "\n",
    "    # Get the starting character index of the first token in the list\n",
    "    start_char = doc[indices[0]].idx\n",
    "\n",
    "    # Get the ending character index of the last token in the list\n",
    "    # This is the starting character index of the last token + its length\n",
    "    end_char = doc[indices[-1]].idx + len(doc[indices[-1]])\n",
    "\n",
    "    return (start_char, end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04dace2",
   "metadata": {},
   "source": [
    "#### Custom data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84310024",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T12:37:02.288867Z",
     "start_time": "2023-12-16T12:37:02.116144Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read triples \n",
    "\n",
    "with open('formatted_triples.pickle', 'rb') as handle: \n",
    "    \n",
    "    triples  = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6345a452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T04:56:21.917637Z",
     "start_time": "2023-11-01T04:56:21.871215Z"
    }
   },
   "outputs": [],
   "source": [
    "random.shuffle(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483db80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T04:56:28.006305Z",
     "start_time": "2023-11-01T04:56:27.454867Z"
    },
    "code_folding": [
     30
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Format data \n",
    "\n",
    "custom_rows = []\n",
    "for triple in tqdm(triples[:30000]): \n",
    "\n",
    "    try: \n",
    "        doc = triple['sentence']\n",
    "        text = doc.text\n",
    "\n",
    "        subj_start_end = get_span(doc, triple['subj'])\n",
    "        subj_start_end = (subj_start_end[0], subj_start_end[1], 'subj')\n",
    "\n",
    "        rel_start_end = get_span(doc, triple['rel'])\n",
    "        rel_start_end = (rel_start_end[0], rel_start_end[1], 'rel')\n",
    "\n",
    "        obj_start_end = get_span(doc, triple['obj'])\n",
    "        obj_start_end = (obj_start_end[0], obj_start_end[1], 'obj')\n",
    "        \n",
    "        entities_dict = {\"entities\":[subj_start_end, rel_start_end, obj_start_end]}\n",
    "        \n",
    "        for mod_indices in triple['mods']: \n",
    "            \n",
    "            mod_start_end = get_span(doc, mod_indices)\n",
    "            mod_start_end = (mod_start_end[0], mod_start_end[1], 'mod')\n",
    "            entities_dict['entities'].append(mod_start_end)\n",
    "\n",
    "        row = (text, entities_dict)\n",
    "\n",
    "        custom_rows.append(row)\n",
    "        \n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "random.shuffle(custom_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9af13",
   "metadata": {},
   "source": [
    "#### Train Model \n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2022/06/custom-named-entity-recognition-using-spacy-v3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52209f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T05:01:40.612266Z",
     "start_time": "2023-11-01T05:01:37.732620Z"
    },
    "code_folding": [
     5
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup dataset to train \n",
    "\n",
    "db = DocBin()\n",
    "fails = []\n",
    "index = 0\n",
    "for text, annotations in tqdm(custom_rows[:20000]): \n",
    "    \n",
    "    try:\n",
    "        text = text.encode('ascii', 'ignore').decode(\"utf-8\") \n",
    "        doc = nlp(text)\n",
    "\n",
    "        ents = []\n",
    "        for start, end, label in annotations['entities']: \n",
    "\n",
    "            span = doc.char_span(start, end, label = label)\n",
    "            ents.append(span)\n",
    "\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "        \n",
    "    except:\n",
    "        fails.append(index)\n",
    "        \n",
    "    index += 1\n",
    "\n",
    "db.to_disk(\"./train.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66b4f908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T12:40:00.302332Z",
     "start_time": "2023-12-16T12:39:57.484970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\r\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\r\n",
      "config.cfg\r\n",
      "You can now add your data and train your pipeline:\r\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f5b3f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-16T13:08:06.159917Z",
     "start_time": "2023-12-16T12:40:04.972574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: test_2\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: test_2\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0         817.84   1014.31    0.09    0.05    0.44    0.00\n",
      "  0     200      309343.13  82478.86   71.19   73.92   68.66    0.71\n",
      "  1     400       37178.25  12138.18   90.17   90.74   89.61    0.90\n",
      "  1     600       10779.26   6000.93   94.73   95.16   94.30    0.95\n",
      "  2     800       10015.66   4049.01   95.35   95.13   95.58    0.95\n",
      "  3    1000        9575.70   3563.86   96.31   96.61   96.01    0.96\n",
      "  3    1200        5475.85   2912.88   96.91   97.07   96.74    0.97\n",
      "  4    1400        3421.96   2131.85   97.52   97.67   97.37    0.98\n",
      "  5    1600        3529.32   2072.46   97.85   97.85   97.86    0.98\n",
      "  5    1800        5287.74   2010.16   98.30   98.30   98.30    0.98\n",
      "  6    2000        3157.69   1586.99   98.53   98.66   98.40    0.99\n",
      "  7    2200        2442.65   1420.29   98.41   98.45   98.37    0.98\n",
      "  7    2400        4282.06   1460.99   98.70   98.69   98.71    0.99\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./test_2 --paths.train ./train.spacy --paths.dev ./train.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d35874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
